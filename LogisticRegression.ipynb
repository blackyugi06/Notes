{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Multivariate Logistic Regression Implementation </h2>"
      ],
      "metadata": {
        "id": "5lqEWp3gxTej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "oDyQpRe8uPNY",
        "outputId": "3344a81b-2c76-445a-eb70-51f86a257aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'heart-disease-prediction-using-logistic-regression' dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/heart-disease-prediction-using-logistic-regression'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "kagglehub.dataset_download(\"dileep070/heart-disease-prediction-using-logistic-regression\")\n",
        "# Load dataset\n",
        "path = '/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv'\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable\n",
        "target = 'TenYearCHD'\n",
        "X = df.drop(target, axis = 1)\n",
        "y = df[target].copy()\n",
        "\n",
        "# Seperate train and test sets\n",
        "X_train = X[:int(0.9 * len(X))].to_numpy()\n",
        "X_test = X[int(0.9 * len(X)):].to_numpy()\n",
        "y_train = y[:int(0.9 * len(y))].to_numpy()\n",
        "y_test = y[int(0.9 * len(y)):].to_numpy()\n",
        "\n",
        "# Standerdize X\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "rbBpLqwiu5_N"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, lr : float = 0.01, input_dim : int = 1) -> None:\n",
        "        self.lr = lr\n",
        "        self.input_dim = input_dim\n",
        "        self.w = np.random.rand(input_dim)\n",
        "        self.b = np.random.rand()\n",
        "        return\n",
        "\n",
        "    def __sigmoid__(self, x : np.ndarray) -> np.ndarray:\n",
        "        x = np.clip(x, -250, 250)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def forward(self, X : np.ndarray) -> np.ndarray:\n",
        "        y = self.__sigmoid__(X @ self.w.T + self.b)\n",
        "        return y\n",
        "\n",
        "    def __optimize__(self, grad_w : np.ndarray, grad_b : np.float64):\n",
        "        self.b -= self.lr * grad_b\n",
        "        self.w -= self.lr * grad_w\n",
        "\n",
        "    def fit(self, X : np.ndarray, y : np.ndarray, epochs : int, X_test : np.ndarray, y_test : np.ndarray) -> None:\n",
        "        # Avoid nans can be difficult to debug\n",
        "        X = np.nan_to_num(X)\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            # predict train data\n",
        "            preds = self.forward(X)\n",
        "            # get gradients\n",
        "            grad_w = (1/len(X)) * X.T @ (preds - y)\n",
        "            grad_b = (preds - y).mean()\n",
        "            # get loss\n",
        "            loss = -(y * np.log((preds >= 0.5) + 1e-9) + (1-y)*np.log(1 - (preds >= 0.5) + 1e-9)).mean()\n",
        "            pred_test = self.forward(X_test)\n",
        "            # get accuracy on test data\n",
        "            acc_test = ((pred_test >= 0.5) == y_test).mean()\n",
        "            # tune parameters\n",
        "            self.__optimize__(grad_w, grad_b)\n",
        "            if epoch % 20 == 0:\n",
        "                print(f'Epoch {epoch} / {epochs} Train: {loss} Accuracy: {acc_test}')\n",
        "\n",
        "model = LogisticRegression(input_dim = X_train.shape[1])\n",
        "model.fit(X_train, y_train, 1000, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3A0lKUI1akN",
        "outputId": "628f1940-7b97-4643-e8fb-5c2383d8fb25"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 / 1000 Train: 9.50314419159787 Accuracy: 0.5259433962264151\n",
            "Epoch 40 / 1000 Train: 9.34014000304559 Accuracy: 0.5330188679245284\n",
            "Epoch 60 / 1000 Train: 9.204303179252024 Accuracy: 0.5377358490566038\n",
            "Epoch 80 / 1000 Train: 9.101067193168916 Accuracy: 0.5377358490566038\n",
            "Epoch 100 / 1000 Train: 8.88916174805095 Accuracy: 0.5400943396226415\n",
            "Epoch 120 / 1000 Train: 8.69899019473996 Accuracy: 0.5471698113207547\n",
            "Epoch 140 / 1000 Train: 8.574020316849879 Accuracy: 0.5518867924528302\n",
            "Epoch 160 / 1000 Train: 8.449050438959798 Accuracy: 0.5566037735849056\n",
            "Epoch 180 / 1000 Train: 8.324080561069717 Accuracy: 0.5613207547169812\n",
            "Epoch 200 / 1000 Train: 8.150209426613953 Accuracy: 0.5683962264150944\n",
            "Epoch 220 / 1000 Train: 7.905703143785534 Accuracy: 0.5754716981132075\n",
            "Epoch 240 / 1000 Train: 7.704664644571057 Accuracy: 0.5943396226415094\n",
            "Epoch 260 / 1000 Train: 7.492759199453094 Accuracy: 0.6084905660377359\n",
            "Epoch 280 / 1000 Train: 7.2482529166246765 Accuracy: 0.625\n",
            "Epoch 300 / 1000 Train: 7.041780944458455 Accuracy: 0.6297169811320755\n",
            "Epoch 320 / 1000 Train: 6.813575080485265 Accuracy: 0.6533018867924528\n",
            "Epoch 340 / 1000 Train: 6.579935743560331 Accuracy: 0.6627358490566038\n",
            "Epoch 360 / 1000 Train: 6.36803029844237 Accuracy: 0.6745283018867925\n",
            "Epoch 380 / 1000 Train: 6.1452579074209215 Accuracy: 0.6863207547169812\n",
            "Epoch 400 / 1000 Train: 5.9387859352547006 Accuracy: 0.6933962264150944\n",
            "Epoch 420 / 1000 Train: 5.75404785489545 Accuracy: 0.7028301886792453\n",
            "Epoch 440 / 1000 Train: 5.509541572067032 Accuracy: 0.7240566037735849\n",
            "Epoch 460 / 1000 Train: 5.324803491707783 Accuracy: 0.7334905660377359\n",
            "Epoch 480 / 1000 Train: 5.13463193839679 Accuracy: 0.7476415094339622\n",
            "Epoch 500 / 1000 Train: 4.971627749844511 Accuracy: 0.7523584905660378\n",
            "Epoch 520 / 1000 Train: 4.770589250630035 Accuracy: 0.7617924528301887\n",
            "Epoch 540 / 1000 Train: 4.67822021045041 Accuracy: 0.7665094339622641\n",
            "Epoch 560 / 1000 Train: 4.515216021898131 Accuracy: 0.7665094339622641\n",
            "Epoch 580 / 1000 Train: 4.3685122522010795 Accuracy: 0.7759433962264151\n",
            "Epoch 600 / 1000 Train: 4.189207644793572 Accuracy: 0.7783018867924528\n",
            "Epoch 620 / 1000 Train: 4.064237766903491 Accuracy: 0.7806603773584906\n",
            "Epoch 640 / 1000 Train: 3.9610017808203817 Accuracy: 0.785377358490566\n",
            "Epoch 660 / 1000 Train: 3.825164957026816 Accuracy: 0.7877358490566038\n",
            "Epoch 680 / 1000 Train: 3.79256411931636 Accuracy: 0.7971698113207547\n",
            "Epoch 700 / 1000 Train: 3.7164954979919633 Accuracy: 0.8042452830188679\n",
            "Epoch 720 / 1000 Train: 3.624126457812338 Accuracy: 0.8066037735849056\n",
            "Epoch 740 / 1000 Train: 3.553491309439684 Accuracy: 0.8066037735849056\n",
            "Epoch 760 / 1000 Train: 3.499156579922258 Accuracy: 0.8066037735849056\n",
            "Epoch 780 / 1000 Train: 3.4611222692600596 Accuracy: 0.8136792452830188\n",
            "Epoch 800 / 1000 Train: 3.3904871208874052 Accuracy: 0.8136792452830188\n",
            "Epoch 820 / 1000 Train: 3.3959205938391475 Accuracy: 0.8136792452830188\n",
            "Epoch 840 / 1000 Train: 3.3633197561286914 Accuracy: 0.8113207547169812\n",
            "Epoch 860 / 1000 Train: 3.3470193372734642 Accuracy: 0.8160377358490566\n",
            "Epoch 880 / 1000 Train: 3.357886283176949 Accuracy: 0.8136792452830188\n",
            "Epoch 900 / 1000 Train: 3.330718918418236 Accuracy: 0.8207547169811321\n",
            "Epoch 920 / 1000 Train: 3.314418499563008 Accuracy: 0.8207547169811321\n",
            "Epoch 940 / 1000 Train: 3.2872511348042948 Accuracy: 0.8207547169811321\n",
            "Epoch 960 / 1000 Train: 3.254650297093839 Accuracy: 0.8207547169811321\n",
            "Epoch 980 / 1000 Train: 3.216615986431641 Accuracy: 0.8231132075471698\n",
            "Epoch 1000 / 1000 Train: 3.2166159864316404 Accuracy: 0.8254716981132075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JV1Hqc0AI5AK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}